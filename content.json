{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"John Doe","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"ceph部署及使用","slug":"ceph部署及使用","date":"2019-04-30T01:24:53.000Z","updated":"2019-04-30T01:26:04.091Z","comments":true,"path":"2019/04/30/ceph部署及使用/","link":"","permalink":"http://yoursite.com/2019/04/30/ceph部署及使用/","excerpt":"","text":"设备清单 192.168.6.156 node1(OSD MON)192.168.6.157 node2(OSD MON)192.168.6.158 node3(OSD MON) 前置准备 安装ansibleyum -y install ansiblevi /etc/ansible/hosts[node]192.168.6.156192.168.6.157192.168.6.158 配置hosts[root@admin-node ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.6.155 admin-node192.168.6.156 node1192.168.6.157 node2192.168.6.158 node3 配置互信ssh-keygen -t rsassh-copy-id -i ~/.ssh/id_rsa.pub root@node1 批量命令示例：ansible node -m copy -a ‘src=/etc/hosts dest=/etc/‘ansible node -a ‘systemctl stop firewalld’ 安装必要软件yum install -y yum-utilsyum-config-manager –add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/yum install –nogpgcheck -y epel-releaserpm –import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7rm -f /etc/yum.repos.d/dl.fedoraproject.org* [root@localhost yum.repos.d]# cat ceph.repo[ceph]name=Ceph noarch packagesbaseurl=http://download.ceph.com/rpm-mimic/el7/x86_64enabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.asc scp /etc/hosts 192.168.6.156:/etc/scp /etc/yum.repos.d/ceph.repo 192.168.6.158:/etc/yum.repos.d/ scp /etc/yum.repos.d/ceph.repo node3:/etc/yum.repos.d/ceph.repo yum install -y ntpdate 1 /2 /sbin/ntpdate -u 192.168.6.130 &gt;/dev/null 2&gt;&amp;1 yum -y install ceph-deploy 修改主机名：vi /etc/hostname hostname admin-node 部署集群各主机执行useradd ceph-adminecho “ceph-admin ALL = (root) NOPASSWD:ALL” | tee /etc/sudoers.d/ceph-adminchmod 0440 /etc/sudoers.d/ceph-admin 无密码登录配置： deploy 主机执行su - ceph-adminssh-keygen将 deploy 主机上 id_rsa.pub 复制至各节点：/home/ceph-admin/.ssh/authorized_keyschmod 400 authorized_keys 先在管理节点上创建一个目录，用于保存 ceph-deploy 生成的配置文件和密钥对 cd my-clusterceph-deploy new node1修改副本数把下面这行加入 [global] 段：osd pool default size = 2 安装 ceph-deployyum -y install ceph-deploy 启动一个新的ceph集群ceph-deploy new –cluster-network 192.168.6.0/24 –public-network 192.168.6.0/24 admin-node node{1,2,3} 部署 mimic 版本ceph集群ceph-deploy install –release mimic admin-node node{1,2,3} –no-adjust-repos 代理网络配置：[ceph-admin@admin-node ~]$ cat .ssh/environmentexport http_proxy=”192.168.6.130:8000”export https_proxy=”192.168.6.130:8000”export no_proxy=192.168.6.*,127.0.0.1 cd my-clusterceph-deploy install admin-node node1 node2 node3 –no-adjust-repos 激活监控节点ceph-deploy –overwrite-conf mon create-initial 管理机配置其实就是将管理 key 放到管理机 /etc/ceph 目录。yum install ceph ceph-deploy admin admin-nodesudo chmod 644 /etc/ceph/ceph.client.admin.keyring [ceph-admin@admin-node my-cluster]$ ceph healthHEALTH_OK ceph-deploy osd create node2 –data /data 创建 ceph 管理进程服务ceph-deploy mgr create admin-node [ceph-admin@admin-node my-cluster]$ ps -ef|grep mgrpostfix 1191 1189 0 Jan31 ? 00:00:00 qmgr -l -t unix -uceph 21599 1 1 16:01 ? 00:00:00 /usr/bin/ceph-mgr -f –cluster ceph –id admin-node –setuser ceph –setgroup cephceph-ad+ 21661 19841 0 16:02 pts/0 00:00:00 grep –color=auto mgr [ceph-admin@admin-node my-cluster]$ ceph -s cluster: id: 63077366-425b-46f3-b9f7-ce214a13403f health: HEALTH_OK services: mon: 3 daemons, quorum node1,node2,node3 mgr: admin-node(active) osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: 启动osd创建数据[ceph-admin@admin-node my-cluster]$ ceph-deploy osd create –data /dev/sdb node{1,2,3} 集群启停启动systemctl start ceph.target 停止systemctl stop ceph*.service ceph*.target 根据进程类型启停systemctl start ceph-osd.targetsystemctl start ceph-mon.targetsystemctl start ceph-mds.target systemctl stop ceph-mon*.service ceph-mon.targetsystemctl stop ceph-osd*.service ceph-osd.targetsystemctl stop ceph-mds*.service ceph-mds.target 移除节点ceph-deploy purge admin-nodeceph-deploy purgedata admin-node 删除镜像https://bugzilla.redhat.com/show_bug.cgi?id=1561758存在 bugrbd: error: image still has watchersThis means the image is still open or the client using it crashed. Try again after closing/unmapping it or waiting 30s for the crashed client to timeout. 创建rgw网关RADOSGW：RADOSGW是一套基于当前流行的RESTFUL协议的网关，并且兼容S3和Swift ceph-deploy install –rgw node1 –no-adjust-repos ceph-deploy gatherkeys node1 ceph-deploy rgw create node1 ISCSI 网关 升级系统内核至4.16升级后切换内核grub2-set-default 0grub2-editenv list重启服务器 http://docs.ceph.com/docs/mimic/rbd/iscsi-target-ansible/ 至少两节点做网关，做网关的节点上要有 iscsi-gateway.cfg ceph.client.admin.keyring 文件scp /home/ceph-admin/my-cluster/ceph.bootstrap-rgw.keyring ceph1:/etc/ceph/ 在线更新配置(默认好像是 20，6)ceph tell osd. config set osd_heartbeat_grace 20ceph tell osd. config set osd_heartbeat_interval 5ceph tell osd.* config set osd_client_watch_timeout 15 写入配置文件：osd_heartbeat_grace = 20osd_heartbeat_interval = 5osd_client_watch_timeout = 15 http://docs.ceph.com/docs/master/rbd/iscsi-target-cli-manual-install/https://ceph.com/planet/ceph%E7%9A%84iscsi-gateway/http://docs.ceph.com/docs/mimic/rbd/iscsi-initiator-esx/ 配置参考：https://blog.51cto.com/candon123/2125049 ceph osd pool create rbd 10 10ceph osd pool application enable rbd rbd[root@ceph1 ~]# ceph osd pool create rbd 16 16pool ‘rbd’ created 创建ISCSI网关gwclicd iscsi-targetcreate iqn.2003-01.com.redhat.iscsi-gw:iscsi-igwcreate node1 192.168.6.156 skipchecks=truecreate node2 192.168.6.157 skipchecks=true 配置用户密码/iscsi-target…1eba-7245c996&gt; auth esxiuser/esxiuser_123 创建RBD imageRBD的使用实际上就是对RBD image的使用，RBD image是由多个对象条带化存储到后端的块设备。查看块设备的命令是rbd info {pool-name}/{image-name} /iscsi-target&gt; cd /disks/disks&gt; create rbd vol01 1G; #格式 create pool=rbd image= vol01 size=1G esxi中获取/iscsi-target…csi-igw/hosts&gt; create iqn.1998-01.com.vmware:5bced6fd-5645-4a6a-cf73-b88303541eba-7245c996/iscsi-target…1eba-7245c996&gt; disk add rbd.vol01 最终效果： esxi使用cephhttp://docs.ceph.com/docs/mimic/rbd/iscsi-initiator-esx/ 需在esxi主机上执行此命令，web前台配置不生效。[root@localhost:~] esxcli iscsi adapter auth chap set –direction=uni –authname=esxiuser –secret=esxiuser_123 –level=discouraged -A vmhba68客户端挂载 ceph 文件系统 创建MDSceph-deploy mds create admin-node ceph-deploy mds create ceph{1,2,3}使用文件系统功能时，必须要有元数据服务器。 创建 poolCeph文件系统至少需要两个RADOS池，一个用于数据，一个用于元数据 ceph osd pool create cephfs_data 128pool ‘cephfs_data’ created ceph osd pool create cephfs_metadata 64pool ‘cephfs_metadata’ created 创建文件系统[ceph-admin@admin-node my-cluster]$ ceph fs new cephfs cephfs_metadata cephfs_datanew fs with metadata pool 2 and data pool 1 查看FS[ceph-admin@admin-node my-cluster]$ ceph fs lsname: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ] [ceph-admin@admin-node my-cluster]$ ceph mds statcephfs-1/1/1 up {0=admin-node=up:active} 使用文件系统yum install ceph-common 加载模块[root@admin-node my-cluster]# lsmod |grep rbd[root@admin-node my-cluster]# modprobe rbd[root@admin-node my-cluster]# lsmod |grep rbdrbd 83889 0libceph 282661 1 rbd [root@admin-node my-cluster]# mkdir /mnt/mycephfs 获取key[root@admin-node my-cluster]# ceph auth get-key client.adminAQAn+VNc/gPhIBAAUvZ8+IMFvDlUP9wJP19duw== 挂载mount -t ceph node1:6789,node2:6789,node3:6789:/ /mnt/mycephfs -o name=admin,secret=AQAn+VNc/gPhIBAAUvZ8+IMFvDlUP9wJP19duw== 删除 cephfs在各节点执行： systemctl stop ceph-mds@ceph1.service不同节点 @ 后内容不一样。ceph fs rm cephfs –yes-i-really-mean-it 删除poolmon_allow_pool_delete = true 添加至 /etc/ceph/ceph.conf [global]重启 systemctl restart ceph-mon@ceph1[root@ceph1 ~]# ceph osd pool delete cephfs_data cephfs_data –yes-i-really-really-mean-itpool ‘cephfs_data’ removed ceph osd pool delete cephfs_metadata cephfs_metadata –yes-i-really-really-mean-it k8s使用rbd 创建poolceph osd pool create k8s-rbd 64 64 #查看 poolceph osd lspools 设置配额ceph osd pool set-quota k8s-rbd max_bytes 2048000000000要取消配额，设置为 0 创建镜像rbd create –size 10240 k8s-rbd/data #10GB [root@ceph1 ~]# ceph health detailHEALTH_WARN application not enabled on 1 pool(s)POOL_APP_NOT_ENABLED application not enabled on 1 pool(s) application not enabled on pool ‘k8s-rbd’ use ‘ceph osd pool application enable ‘, where is ‘cephfs’, ‘rbd’, ‘rgw’, or freeform for custom applications.[root@ceph1 ~]# ceph osd pool application enable k8s-rbd k8senabled application ‘k8s’ on pool ‘k8s-rbd’cephfs用户管理 list userceph auth list add usermds caps 可以针对路径进行授权, 但是如果两个目录存储在同一个 pool 上的话, 即便针对 path 授权似乎也没什么效果( hammer 不支持mds caps 中的 path 设置).osd caps 可以针对 pool 进行授权, 这个还是比较好使的, 但是要注意与前面的 path 保持一致. [root@admin-node ceph]# ceph auth get-or-create client.lubx mon ‘allow r’ mds ‘allow r,allow rw path=/lubx’ osd ‘allow rw pool=cephfs_data’[client.lubx] key = AQBljnxcERWgJRAAlJMT2Un9FfTjBBMEJ6ucNg== 修改权限ceph auth caps client.lubx mds ‘allow r path=/lubx’ mon ‘allow r’ osd ‘allow rw pool=cephfs_data’ 集群监控 启动dashboardceph mgr module enable dashboard 查看moduleceph mgr module ls 生成证书ceph dashboard create-self-signed-cert 创建用户ceph dashboard set-login-credentials admin admin 查看服务ceph mgr services检查状态 集群状态ceph -s –检查状态ceph df –查看空间 集群配置查看ceph daemon /var/run/ceph/ceph-mon.*.asok config show|grep rbd_default mon状态ceph mon stat –查看mon相关信息ceph quorum_status –查看选举状态{“election_epoch”:4,”quorum”:[0,1,2],”quorum_names”:[“node1”,”node2”,”node3”],”quorum_leader_name”:”node1”,”monmap”:{“epoch”:1,”fsid”:”63077366-425b-46f3-b9f7-ce214a13403f”,”modified”:”2019-02-01 15:45:37.066998”,”created”:”2019-02-01 15:45:37.066998”,”features”:{“persistent”:[“kraken”,”luminous”,”mimic”,”osdmap-prune”],”optional”:[]},”mons”:[{“rank”:0,”name”:”node1”,”addr”:”192.168.6.156:6789/0”,”public_addr”:”192.168.6.156:6789/0”},{“rank”:1,”name”:”node2”,”addr”:”192.168.6.157:6789/0”,”public_addr”:”192.168.6.157:6789/0”},{“rank”:2,”name”:”node3”,”addr”:”192.168.6.158:6789/0”,”public_addr”:”192.168.6.158:6789/0”}]}} ceph mon dump –查看 mon 映射信息dumped monmap epoch 1epoch 1fsid 63077366-425b-46f3-b9f7-ce214a13403flast_changed 2019-02-01 15:45:37.066998created 2019-02-01 15:45:37.0669980: 192.168.6.156:6789/0 mon.node11: 192.168.6.157:6789/0 mon.node22: 192.168.6.158:6789/0 mon.node3 sudo ceph daemon mon.node1 mon_status –node1 节点执行查看详细信息 osd状态 ceph osd stat ##查看 osd 运行状态ceph osd dump ##查看 osd 映射信息ceph osd perf ##查看数据延迟ceph osd df ##详细列出集群每块磁盘的使用情况ceph osd tree ##查看 osd 目录树ceph osd getmaxosd ##查看最大 osd 的个数 PG信息ceph pg dumpceph pg stat LVM分区OSD创建数据时会自动创建LVM分区，无需手动创建。 fdisk /dev/sdbpvcreate /dev/sdb1vgcreate vg01 /dev/sdb1lvcreate -l 100%FREE -n data vg01mkfs.xfs /dev/vg01/datablkid /dev/vg01/datavim /etc/fstab 清理集群配置及数据ceph-deploy uninstall admin-node node{1,2,3} ceph-deploy purgedata admin-node node{1,2,3} ceph-deploy forgetkeys 参数设置 /etc/ceph/ceph.conf 已有配置及默认配置[root@ceph1 ~]# ceph –show-config|grep mon_allow_pool_deletemon_allow_pool_delete = true 方式1：daemon方式，重启进程后参数丢失 查看正在运行参数ceph –admin-daemon /var/run/ceph/ceph-osd.0.asok config show|grep ‘rbd’ 查看[root@ceph1 ~]# ceph daemon osd.0 config get mon_osd_full_ratio{ “mon_osd_full_ratio”: “0.950000”} 修改ceph daemon osd.1 config set mon_osd_full_ratio 0.97 方式2： 集群批量修改ceph tell mon. injectargs ‘–mon_osd_report_timeout 400’ceph tell osd. injectargs ‘–rbd_cache_max_dirty_age = 1’ 分发配置文件修改配置su - ceph-admincd my-cluster修改 ceph.conf 配置 ceph-deploy –overwrite-conf config push ceph{1..3} 修改副本数[root@ceph1 ~]# ceph osd pool get k8s-rbd sizesize: 3[root@ceph1 ~]# ceph osd pool set k8s-rbd size 2set pool 5 size to 2参考文档http://blog.51cto.com/wangzhijian/2156186https://www.cnblogs.com/luohaixian/p/8087591.html 问题 执行 ceph-deploy 报错[ceph-admin@deploy my-cluster]$ ceph-deployTraceback (most recent call last):File “/bin/ceph-deploy”, line 18, in from ceph_deploy.cli import mainFile “/usr/lib/python2.7/site-packages/ceph_deploy/cli.py”, line 1, in import pkg_resourcesImportError: No module named pkg_resources[ceph-admin@deploy my-cluster]$ exit 执行：yum install python-setuptools PG数量问题[ceph-admin@admin-node my-cluster]$ ceph osd pool create cephfs_data 128pool ‘cephfs_data’ created[ceph-admin@admin-node my-cluster]$ ceph osd pool create cephfs_metadata 128Error ERANGE: pg_num 128 size 3 would mean 768 total pgs, which exceeds max 750 (mon_max_pg_per_osd 250 * num_in_osds 3) [ceph-admin@admin-node my-cluster]$ ceph osd pool get cephfs_data pg_numpg_num: 128 no active mgr 问题 cd /etc/cephceph-deploy mgr create admin-node [root@admin-node ceph]# ceph -s cluster: id: 63077366-425b-46f3-b9f7-ce214a13403f health: HEALTH_OK 存在一个问题：/usr/bin/ceph-mgr 进程卡死，需要手工 kill -9。再执行ceph-deploy mgr create admin-node 才能正常。","categories":[],"tags":[]},{"title":"my-first-blog","slug":"my-first-blog","date":"2019-04-29T04:26:38.000Z","updated":"2019-04-29T04:28:43.811Z","comments":true,"path":"2019/04/29/my-first-blog/","link":"","permalink":"http://yoursite.com/2019/04/29/my-first-blog/","excerpt":"This is a test page! 前言使用github pages服务搭建博客的好处有： 全是静态文件，访问速度快； 免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台； 可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的；","text":"This is a test page! 前言使用github pages服务搭建博客的好处有： 全是静态文件，访问速度快； 免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台； 可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的； 数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行； 博客内容可以轻松打包、转移、发布到其它平台； 等等；","categories":[],"tags":[{"name":"技术","slug":"技术","permalink":"http://yoursite.com/tags/技术/"}]},{"title":"test_my_site","slug":"test-my-site","date":"2019-04-29T03:31:43.000Z","updated":"2019-04-29T03:31:43.208Z","comments":true,"path":"2019/04/29/test-my-site/","link":"","permalink":"http://yoursite.com/2019/04/29/test-my-site/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2019-04-29T03:29:33.760Z","updated":"2019-04-29T03:29:33.760Z","comments":true,"path":"2019/04/29/hello-world/","link":"","permalink":"http://yoursite.com/2019/04/29/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}